{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanibasanth/pavani_INFO5731_Fall2024/blob/main/Kommineni_Pavani_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install webdriver_manager\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install pandas nltk\n",
        "\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb && apt install ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qczjU5I2RVp",
        "outputId": "be6cf96f-030a-4a8d-ac44-1759af572a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.8.30)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.2\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,027 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,447 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,596 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,361 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,592 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [54.3 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,318 kB]\n",
            "Fetched 19.9 MB in 2s (8,249 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 1s (41.1 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123614 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123822 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124052 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "--2024-10-01 22:14:33--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.132.93, 74.125.132.91, 74.125.132.190, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.132.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111860972 (107M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 106.68M   190MB/s    in 0.6s    \n",
            "\n",
            "2024-10-01 22:14:34 (190 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [111860972/111860972]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'google-chrome-stable' instead of './google-chrome-stable_current_amd64.deb'\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 3 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 10.9 MB/123 MB of archives.\n",
            "After this operation, 417 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.2 [10.7 MB]\n",
            "Get:3 /content/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 129.0.6668.70-1 [112 MB]\n",
            "Fetched 10.9 MB in 1s (9,809 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 124080 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "Preparing to unpack .../google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (129.0.6668.70-1) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up google-chrome-stable (129.0.6668.70-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "Google_Drive_Path = '/content/drive/MyDrive/Top_1000_Furiosa_A_Mad_Max-Saga_IMdB_Reviews.csv'\n",
        "\n",
        "# Define driver setup function\n",
        "def driversetup():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument(\"lang=en\")\n",
        "    options.add_argument(\"start-maximized\")\n",
        "    options.add_argument(\"disable-infobars\")\n",
        "    options.add_argument(\"--disable-extensions\")\n",
        "    options.add_argument(\"--incognito\")\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\")\n",
        "\n",
        "    return driver\n",
        "\n",
        "# Initialize the driver\n",
        "driver = driversetup()\n",
        "\n",
        "# Open the IMDb reviews page\n",
        "url = \"https://www.imdb.com/title/tt12037194/reviews/?ref_=tt_ov_ql_2\"\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to load\n",
        "time.sleep(3)\n",
        "\n",
        "# Click the \"All\" button to load all reviews\n",
        "try:\n",
        "\n",
        "\n",
        "    for i in range(40):\n",
        "      try:\n",
        "          css_selector = 'load-more-trigger'\n",
        "          driver.find_element(By.ID, css_selector).click()\n",
        "          time.sleep(3)  # Wait for reviews to load\n",
        "      except Exception as e:\n",
        "          print(f\"Error clicking load-more: {e}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error clicking 'All' button: {e}\")\n",
        "\n",
        "# Parse page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Find all review containers\n",
        "reviews = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "# List to store reviews\n",
        "review_data = []\n",
        "\n",
        "# Loop through the first 1000 reviews and scrape relevant details\n",
        "for i in range(min(1000, len(reviews))):\n",
        "    review = reviews[i]\n",
        "\n",
        "    # Scrape rating (if available)\n",
        "    rating_tag = review.find('span', class_='rating-other-user-rating')\n",
        "    if rating_tag:\n",
        "        rating = rating_tag.text.strip().split('/')[0]\n",
        "    else:\n",
        "        rating = \"No rating\"\n",
        "\n",
        "    # Scrape review summary and content\n",
        "    summary = review.find('a', class_='title').text.strip()\n",
        "    content = review.find('div', class_='text').text.strip()\n",
        "\n",
        "    # Store the scraped review in the list\n",
        "    review_data.append({\n",
        "        'Rating': rating,\n",
        "        'Summary': summary,\n",
        "        'Content': content\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the review data\n",
        "df = pd.DataFrame(review_data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(Google_Drive_Path, index=False)\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "print(\"Top 1000 reviews have been scraped and saved to CSV.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMYk2QETdV8x",
        "outputId": "5c031180-9a0f-4702-b705-02400a4d489c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 1000 reviews have been scraped and saved to CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHjYWLxL4VU8",
        "outputId": "0811cb0b-87a1-4636-e5f9-63d84aa46ea1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b76c5f-cbbf-4cdd-b457-3dbb1493a70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text cleaning complete. Cleaned data has been saved to 'Cleaned Top_1000_Furiosa_A Mad Max Saga IMdB_Reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Load the CSV file\n",
        "data_file = pd.read_csv(Google_Drive_Path)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "word_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to clean text data\n",
        "def text_cleaning(input_text):\n",
        "    # (1) Remove noise (special characters and punctuations)\n",
        "    input_text = re.sub(r'[^\\w\\s]', '', input_text)\n",
        "\n",
        "    # (2) Remove numerical characters\n",
        "    input_text = re.sub(r'\\d+', '', input_text)\n",
        "\n",
        "    # (3) Convert all text to lowercase\n",
        "    input_text = input_text.lower()\n",
        "\n",
        "    # (4) Eliminate stopwords\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    input_text = ' '.join(word for word in input_text.split() if word not in stopwords_set)\n",
        "\n",
        "    # (5) Perform lemmatization\n",
        "    input_text = ' '.join(word_lemmatizer.lemmatize(word) for word in input_text.split())\n",
        "\n",
        "    return input_text\n",
        "\n",
        "# Apply the text_cleaning function to the 'Summary' and 'Content' columns\n",
        "data_file['Cleaned_Summary'] = data_file['Summary'].apply(text_cleaning)\n",
        "data_file['Cleaned_Content'] = data_file['Content'].apply(text_cleaning)\n",
        "\n",
        "# Define path to save the cleaned data\n",
        "cleaned_data_filepath = '/content/drive/MyDrive/Cleaned_Top_1000_Furiosa_A_Mad_Max-Saga_IMdB_Reviews.csv'\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "data_file.to_csv(cleaned_data_filepath, index=False)\n",
        "\n",
        "print(\"Text cleaning complete. Cleaned data has been saved to 'Cleaned Top_1000_Furiosa_A Mad Max Saga IMdB_Reviews.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lJYFMn8E90d",
        "outputId": "ce5bbeaa-504b-4183-8cda-c67f609838e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp_model = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def text_analysis(input_text):\n",
        "    # 1. Parts of Speech (POS) Tagging\n",
        "    word_tokens = word_tokenize(input_text)\n",
        "    pos_labels = pos_tag(word_tokens)\n",
        "\n",
        "    pos_summary = Counter(label for word, label in pos_labels)\n",
        "    print(\"POS Tagging Overview:\")\n",
        "    print(f\"Nouns: {pos_summary['NN'] + pos_summary['NNS'] + pos_summary['NNP'] + pos_summary['NNPS']}\")\n",
        "    print(f\"Verbs: {pos_summary['VB'] + pos_summary['VBD'] + pos_summary['VBG'] + pos_summary['VBN'] + pos_summary['VBP'] + pos_summary['VBZ']}\")\n",
        "    print(f\"Adjectives: {pos_summary['JJ'] + pos_summary['JJR'] + pos_summary['JJS']}\")\n",
        "    print(f\"Adverbs: {pos_summary['RB'] + pos_summary['RBR'] + pos_summary['RBS']}\")\n",
        "\n",
        "    # 2. Constituency Parsing and Dependency Parsing\n",
        "    print(\"\\nConstituency Parsing Result:\")\n",
        "    for sent in sent_tokenize(input_text):\n",
        "        tokenized_sentence = word_tokenize(sent)\n",
        "        tagged_sentence = pos_tag(tokenized_sentence)\n",
        "        parse_tree = nltk.chunk.ne_chunk(tagged_sentence)\n",
        "        print(parse_tree)\n",
        "\n",
        "    print(\"\\nDependency Parsing Result:\")\n",
        "    parsed_doc = nlp_model(input_text)\n",
        "    for sentence in parsed_doc.sents:\n",
        "        for tok in sentence:\n",
        "            print(f\"{tok.text} --{tok.dep_}--> {tok.head.text}\")\n",
        "\n",
        "    # 3. Named Entity Recognition\n",
        "    print(\"\\nNamed Entity Recognition:\")\n",
        "    parsed_doc = nlp_model(input_text)\n",
        "    named_entities = [(entity.text, entity.label_) for entity in parsed_doc.ents]\n",
        "    entity_summary = Counter(label for _, label in named_entities)\n",
        "\n",
        "    print(\"Entity Summary:\")\n",
        "    for entity_label, entity_count in entity_summary.items():\n",
        "        print(f\"{entity_label}: {entity_count}\")\n",
        "\n",
        "    print(\"\\nExtracted Named Entities:\")\n",
        "    for entity_text, entity_label in named_entities:\n",
        "        print(f\"{entity_text} - {entity_label}\")\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "data_frame = pd.read_csv(cleaned_data_filepath)\n",
        "\n",
        "# Analyze the first row of the 'Cleaned_Content' column\n",
        "sample_text_content = data_frame['Cleaned_Content'].iloc[0]\n",
        "text_analysis(sample_text_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LENNjd3QBppL",
        "outputId": "c64391c7-f32d-4d6d-de11-f8115576aac2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging Overview:\n",
            "Nouns: 168\n",
            "Verbs: 60\n",
            "Adjectives: 81\n",
            "Adverbs: 36\n",
            "\n",
            "Constituency Parsing Result:\n",
            "(S\n",
            "  george/NN\n",
            "  miller/RBS\n",
            "  yes/UH\n",
            "  yes/RB\n",
            "  well/RB\n",
            "  sort/VB\n",
            "  anywayi/NNS\n",
            "  really/RB\n",
            "  really/RB\n",
            "  wanted/VBN\n",
            "  love/NN\n",
            "  furiosa/JJ\n",
            "  end/NN\n",
            "  didnt/NN\n",
            "  liked/VBD\n",
            "  didnt/JJ\n",
            "  love/NN\n",
            "  big/JJ\n",
            "  big/JJ\n",
            "  shoe/NN\n",
            "  fill/NN\n",
            "  completely/RB\n",
            "  love/JJ\n",
            "  fury/NN\n",
            "  road/NN\n",
            "  perfect/JJ\n",
            "  action/NN\n",
            "  film/NN\n",
            "  every/DT\n",
            "  way/NN\n",
            "  prepared/JJ\n",
            "  film/NN\n",
            "  fall/NN\n",
            "  shadow/NN\n",
            "  furiosa/NN\n",
            "  fun/NN\n",
            "  sadly/RB\n",
            "  frthe/VBD\n",
            "  good/JJ\n",
            "  news/NN\n",
            "  want/VBP\n",
            "  action/NN\n",
            "  action/NN\n",
            "  load/NN\n",
            "  like/IN\n",
            "  good/JJ\n",
            "  mad/NN\n",
            "  max/NN\n",
            "  story/NN\n",
            "  hold/VBP\n",
            "  true/JJ\n",
            "  promise/NN\n",
            "  entertain/NN\n",
            "  mass/NN\n",
            "  spectacle/NN\n",
            "  glorious/JJ\n",
            "  get/VB\n",
            "  hot/JJ\n",
            "  rod/NN\n",
            "  big/JJ\n",
            "  wheel/NN\n",
            "  digger/NN\n",
            "  bike/IN\n",
            "  shape/NN\n",
            "  size/NN\n",
            "  well/RB\n",
            "  flying/VBG\n",
            "  contraption/NN\n",
            "  weaponry/NN\n",
            "  galore/VBD\n",
            "  holding/VBG\n",
            "  back/RB\n",
            "  violence/NN\n",
            "  explosion/NN\n",
            "  body/NN\n",
            "  flying/VBG\n",
            "  witness/JJ\n",
            "  etc/JJ\n",
            "  plenty/NN\n",
            "  brutal/JJ\n",
            "  nasty/JJ\n",
            "  true/JJ\n",
            "  form/NN\n",
            "  true/JJ\n",
            "  mad/JJ\n",
            "  max/NN\n",
            "  wantcasting/VBG\n",
            "  good/JJ\n",
            "  perhaps/RB\n",
            "  many/JJ\n",
            "  character/NN\n",
            "  leaving/VBG\n",
            "  quite/RB\n",
            "  enough/RB\n",
            "  especially/RB\n",
            "  immortan/JJ\n",
            "  joes/NNS\n",
            "  follower/VB\n",
            "  anja/JJ\n",
            "  taylor/NN\n",
            "  joy/NN\n",
            "  good/JJ\n",
            "  furiosa/NN\n",
            "  hemsworth/NN\n",
            "  seems/VBZ\n",
            "  really/RB\n",
            "  enjoying/JJ\n",
            "  dementus/NN\n",
            "  great/JJ\n",
            "  villain/NN\n",
            "  sort/NN\n",
            "  love/VBP\n",
            "  child/NN\n",
            "  immortan/NN\n",
            "  joe/NN\n",
            "  humungus/NN\n",
            "  presence/NN\n",
            "  wee/NN\n",
            "  speech/NN\n",
            "  villainy/NN\n",
            "  fun/NN\n",
            "  get/VBP\n",
            "  aussie/JJ\n",
            "  need/VBP\n",
            "  hide/JJ\n",
            "  accent/NN\n",
            "  also/RB\n",
            "  liked/VBD\n",
            "  younger/JJR\n",
            "  much/JJ\n",
            "  cleverer/NN\n",
            "  immortan/JJ\n",
            "  joe/NN\n",
            "  film/NN\n",
            "  he/PRP\n",
            "  core/VBD\n",
            "  bad/JJ\n",
            "  guy/NN\n",
            "  without/IN\n",
            "  need/NN\n",
            "  get/VBP\n",
            "  wife/NN\n",
            "  back/RB\n",
            "  motivator/NN\n",
            "  making/NN\n",
            "  crazy/JJ\n",
            "  get/VB\n",
            "  see/JJ\n",
            "  warlord/JJ\n",
            "  smart/JJ\n",
            "  patient/NN\n",
            "  controland/NN\n",
            "  there/RB\n",
            "  world/NN\n",
            "  building/NN\n",
            "  cool/NN\n",
            "  got/VBD\n",
            "  see/NNS\n",
            "  wasteland/VBP\n",
            "  get/VB\n",
            "  hint/NN\n",
            "  tick/NN\n",
            "  made/VBD\n",
            "  sothe/JJ\n",
            "  bad/JJ\n",
            "  news/NN\n",
            "  visually/RB\n",
            "  furiosa/JJ\n",
            "  bit/NN\n",
            "  mixed/JJ\n",
            "  bag/NN\n",
            "  sometimes/RB\n",
            "  get/VB\n",
            "  stuff/JJ\n",
            "  worthy/JJ\n",
            "  predecessor/NN\n",
            "  time/NN\n",
            "  get/VB\n",
            "  stuff/JJ\n",
            "  le/VB\n",
            "  colour/JJ\n",
            "  much/JJ\n",
            "  pale/NN\n",
            "  richness/NN\n",
            "  fr/NN\n",
            "  cinematography/NN\n",
            "  near/IN\n",
            "  standard/JJ\n",
            "  occasionally/RB\n",
            "  cgi/JJ\n",
            "  let/NN\n",
            "  film/NN\n",
            "  bit/VB\n",
            "  fury/JJ\n",
            "  road/NN\n",
            "  got/VBD\n",
            "  impression/NN\n",
            "  almost/RB\n",
            "  practical/JJ\n",
            "  effect/NN\n",
            "  even/RB\n",
            "  wasnt/VBD\n",
            "  furiosa/JJ\n",
            "  sequence/NN\n",
            "  went/VBD\n",
            "  eww/JJ\n",
            "  yikes/NNS\n",
            "  standard/JJ\n",
            "  film/NN\n",
            "  dropped/VBD\n",
            "  moment/NN\n",
            "  preparedthe/NN\n",
            "  music/NN\n",
            "  often/RB\n",
            "  absent/JJ\n",
            "  lacking/VBG\n",
            "  sweeping/JJ\n",
            "  epic/JJ\n",
            "  fury/NN\n",
            "  road/NN\n",
            "  almost/RB\n",
            "  gone/VBN\n",
            "  instead/RB\n",
            "  get/VB\n",
            "  much/JJ\n",
            "  background/NN\n",
            "  stuff/VBD\n",
            "  even/RB\n",
            "  sadly/RB\n",
            "  largely/RB\n",
            "  silent/JJ\n",
            "  action/NN\n",
            "  sequence/NN\n",
            "  lead/JJ\n",
            "  lack/NN\n",
            "  tension/NN\n",
            "  nothing/NN\n",
            "  edge/NN\n",
            "  seat/NN\n",
            "  herethe/NN\n",
            "  writing/VBG\n",
            "  edit/NN\n",
            "  isnt/NN\n",
            "  great/JJ\n",
            "  character/NN\n",
            "  lack/NN\n",
            "  spark/JJ\n",
            "  bit/NN\n",
            "  feel/JJ\n",
            "  missing/VBG\n",
            "  miller/NN\n",
            "  trying/VBG\n",
            "  tell/JJ\n",
            "  story/NN\n",
            "  seems/VBZ\n",
            "  unsure/JJ\n",
            "  write/JJ\n",
            "  character/NN\n",
            "  one/CD\n",
            "  invest/NN\n",
            "  furiosa/NN\n",
            "  ambition/NN\n",
            "  story/NN\n",
            "  telling/VBG\n",
            "  think/JJ\n",
            "  execution/NN\n",
            "  lacking/VBG\n",
            "  core/JJ\n",
            "  story/NN\n",
            "  solid/JJ\n",
            "  seems/VBZ\n",
            "  bit/NN\n",
            "  empty/JJ\n",
            "  screen/NN\n",
            "  film/NN\n",
            "  jump/NN\n",
            "  time/NN\n",
            "  skipping/VBG\n",
            "  character/NN\n",
            "  development/NN\n",
            "  worthy/JJ\n",
            "  explorationin/NN\n",
            "  edit/NN\n",
            "  really/RB\n",
            "  really/RB\n",
            "  missed/VBN\n",
            "  fast/RB\n",
            "  cut/JJ\n",
            "  fury/NN\n",
            "  miller/NN\n",
            "  really/RB\n",
            "  built/VBN\n",
            "  scene/NN\n",
            "  tight/NN\n",
            "  production/NN\n",
            "  cut/VBD\n",
            "  edits/NNS\n",
            "  everything/NN\n",
            "  build/VBP\n",
            "  beautifully/RB\n",
            "  lead/JJ\n",
            "  great/JJ\n",
            "  conclusion/NN\n",
            "  miller/NN\n",
            "  scene/NN\n",
            "  scene/NN\n",
            "  fr/VBP\n",
            "  whereas/IN\n",
            "  furiosa/NNS\n",
            "  really/RB\n",
            "  lacking/VBG\n",
            "  mean/VB\n",
            "  there/RB\n",
            "  much/JJ\n",
            "  tension/NN\n",
            "  many/JJ\n",
            "  close/RB\n",
            "  amazing/VBG\n",
            "  momentsoverall/NN\n",
            "  like/IN\n",
            "  furiosa/NN\n",
            "  good/JJ\n",
            "  mad/NN\n",
            "  max/NN\n",
            "  film/NN\n",
            "  fury/NN\n",
            "  road/NN\n",
            "  id/NN\n",
            "  surprised/VBD\n",
            "  film/NN\n",
            "  recognized/VBN\n",
            "  oscar/NN\n",
            "  time/NN\n",
            "  fury/NN\n",
            "  road/NN\n",
            "  furiosa/NN\n",
            "  might/MD\n",
            "  get/VB\n",
            "  nomination/JJ\n",
            "  two/CD\n",
            "  technical/JJ\n",
            "  nomination/NN\n",
            "  directing/VBG\n",
            "  best/JJS\n",
            "  pictureby/NN\n",
            "  deed/VB\n",
            "  honour/JJ\n",
            "  himby/NN\n",
            "  wallet/NN\n",
            "  honour/NN\n",
            "  george/NN\n",
            "  miller/NNP\n",
            "  witness/NN\n",
            "  furiosa/VBD\n",
            "  worth/JJ\n",
            "  time/NN\n",
            "  money/NN)\n",
            "\n",
            "Dependency Parsing Result:\n",
            "george --compound--> miller\n",
            "miller --nsubj--> wanted\n",
            "yes --intj--> wanted\n",
            "yes --intj--> wanted\n",
            "well --intj--> wanted\n",
            "sort --compound--> anywayi\n",
            "anywayi --nsubj--> wanted\n",
            "really --advmod--> wanted\n",
            "really --advmod--> wanted\n",
            "wanted --ROOT--> wanted\n",
            "love --compound--> furiosa\n",
            "furiosa --compound--> end\n",
            "end --nsubj--> liked\n",
            "did --aux--> liked\n",
            "nt --neg--> liked\n",
            "liked --ccomp--> wanted\n",
            "did --aux--> love\n",
            "nt --neg--> love\n",
            "love --xcomp--> liked\n",
            "big --amod--> fill\n",
            "big --amod--> shoe\n",
            "shoe --compound--> fill\n",
            "fill --dobj--> love\n",
            "completely --advmod--> love\n",
            "love --ccomp--> wanted\n",
            "fury --nmod--> film\n",
            "road --npadvmod--> perfect\n",
            "perfect --amod--> film\n",
            "action --compound--> film\n",
            "film --dobj--> love\n",
            "every --det--> way\n",
            "way --npadvmod--> fun\n",
            "prepared --amod--> film\n",
            "film --compound--> fall\n",
            "fall --compound--> shadow\n",
            "shadow --compound--> fun\n",
            "furiosa --compound--> fun\n",
            "fun --npadvmod--> love\n",
            "sadly --advmod--> want\n",
            "frthe --det--> news\n",
            "good --amod--> news\n",
            "news --nsubj--> want\n",
            "want --ccomp--> wanted\n",
            "action --compound--> action\n",
            "action --compound--> load\n",
            "load --nsubj--> hold\n",
            "like --prep--> load\n",
            "good --amod--> story\n",
            "mad --amod--> story\n",
            "max --compound--> story\n",
            "story --pobj--> like\n",
            "hold --ccomp--> want\n",
            "true --amod--> promise\n",
            "promise --nmod--> spectacle\n",
            "entertain --nmod--> spectacle\n",
            "mass --compound--> spectacle\n",
            "spectacle --nsubj--> get\n",
            "glorious --amod--> spectacle\n",
            "get --nmod--> witness\n",
            "hot --amod--> rod\n",
            "rod --nmod--> size\n",
            "big --amod--> wheel\n",
            "wheel --compound--> digger\n",
            "digger --compound--> size\n",
            "bike --compound--> shape\n",
            "shape --compound--> size\n",
            "size --nmod--> galore\n",
            "well --advmod--> flying\n",
            "flying --amod--> contraption\n",
            "contraption --compound--> weaponry\n",
            "weaponry --compound--> galore\n",
            "galore --dobj--> get\n",
            "holding --acl--> galore\n",
            "back --prt--> holding\n",
            "violence --compound--> explosion\n",
            "explosion --compound--> body\n",
            "body --dobj--> holding\n",
            "flying --amod--> witness\n",
            "witness --nsubj--> etc\n",
            "etc --dep--> wantcasting\n",
            "plenty --amod--> form\n",
            "brutal --amod--> form\n",
            "nasty --amod--> form\n",
            "true --amod--> form\n",
            "form --dobj--> etc\n",
            "true --amod--> max\n",
            "mad --amod--> max\n",
            "max --nsubj--> wantcasting\n",
            "wantcasting --ccomp--> hold\n",
            "good --amod--> character\n",
            "perhaps --advmod--> character\n",
            "many --amod--> character\n",
            "character --nsubj--> leaving\n",
            "leaving --advcl--> wantcasting\n",
            "quite --advmod--> enough\n",
            "enough --advmod--> leaving\n",
            "especially --advmod--> immortan\n",
            "immortan --nsubj--> joes\n",
            "joes --dative--> leaving\n",
            "follower --compound--> joy\n",
            "anja --compound--> joy\n",
            "taylor --compound--> joy\n",
            "joy --dobj--> joes\n",
            "good --compound--> hemsworth\n",
            "furiosa --compound--> hemsworth\n",
            "hemsworth --nsubj--> seems\n",
            "seems --conj--> wanted\n",
            "really --advmod--> enjoying\n",
            "enjoying --xcomp--> seems\n",
            "dementus --npadvmod--> great\n",
            "great --amod--> presence\n",
            "villain --amod--> sort\n",
            "sort --compound--> child\n",
            "love --compound--> child\n",
            "child --compound--> presence\n",
            "immortan --compound--> presence\n",
            "joe --compound--> presence\n",
            "humungus --compound--> presence\n",
            "presence --dobj--> enjoying\n",
            "wee --amod--> speech\n",
            "speech --nmod--> fun\n",
            "villainy --amod--> fun\n",
            "fun --appos--> presence\n",
            "get --compound--> accent\n",
            "aussie --amod--> accent\n",
            "need --compound--> hide\n",
            "hide --compound--> accent\n",
            "accent --nsubj--> liked\n",
            "also --advmod--> liked\n",
            "liked --ccomp--> seems\n",
            "younger --advmod--> much\n",
            "much --amod--> cleverer\n",
            "cleverer --compound--> film\n",
            "immortan --compound--> film\n",
            "joe --compound--> film\n",
            "film --npadvmod--> core\n",
            "he --nsubj--> core\n",
            "core --ccomp--> liked\n",
            "bad --amod--> guy\n",
            "guy --dobj--> core\n",
            "without --prep--> core\n",
            "need --pobj--> without\n",
            "get --conj--> core\n",
            "wife --dobj--> get\n",
            "back --det--> motivator\n",
            "motivator --dep--> wanted\n",
            "making --acl--> motivator\n",
            "crazy --amod--> get\n",
            "get --nsubj--> see\n",
            "see --ccomp--> wanted\n",
            "warlord --dobj--> see\n",
            "smart --amod--> patient\n",
            "patient --compound--> controland\n",
            "controland --ROOT--> controland\n",
            "there --advmod--> controland\n",
            "world --compound--> building\n",
            "building --compound--> cool\n",
            "cool --punct--> controland\n",
            "got --aux--> see\n",
            "see --ROOT--> see\n",
            "wasteland --nsubj--> get\n",
            "get --ccomp--> see\n",
            "hint --compound--> tick\n",
            "tick --nsubj--> made\n",
            "made --ccomp--> see\n",
            "sothe --det--> news\n",
            "bad --amod--> news\n",
            "news --nsubj--> furiosa\n",
            "visually --advmod--> furiosa\n",
            "furiosa --nmod--> bag\n",
            "bit --npadvmod--> mixed\n",
            "mixed --amod--> bag\n",
            "bag --nsubj--> get\n",
            "sometimes --advmod--> get\n",
            "get --ccomp--> made\n",
            "stuff --nmod--> time\n",
            "worthy --amod--> time\n",
            "predecessor --compound--> time\n",
            "time --nsubj--> get\n",
            "get --ccomp--> get\n",
            "stuff --dobj--> get\n",
            "le --advmod--> occasionally\n",
            "colour --advmod--> much\n",
            "much --advmod--> pale\n",
            "pale --amod--> richness\n",
            "richness --pobj--> le\n",
            "fr --compound--> cinematography\n",
            "cinematography --appos--> le\n",
            "near --prep--> cinematography\n",
            "standard --pobj--> near\n",
            "occasionally --advmod--> cgi\n",
            "cgi --npadvmod--> let\n",
            "let --ROOT--> let\n",
            "film --compound--> bit\n",
            "bit --compound--> road\n",
            "fury --compound--> road\n",
            "road --nsubj--> got\n",
            "got --ccomp--> let\n",
            "impression --nmod--> effect\n",
            "almost --advmod--> practical\n",
            "practical --amod--> effect\n",
            "effect --nsubj--> was\n",
            "even --advmod--> was\n",
            "was --ccomp--> got\n",
            "nt --neg--> was\n",
            "furiosa --compound--> sequence\n",
            "sequence --nsubj--> went\n",
            "went --ccomp--> let\n",
            "eww --nsubj--> yikes\n",
            "yikes --advcl--> went\n",
            "standard --amod--> film\n",
            "film --nsubj--> dropped\n",
            "dropped --ccomp--> yikes\n",
            "moment --npadvmod--> absent\n",
            "preparedthe --det--> music\n",
            "music --nsubj--> absent\n",
            "often --advmod--> absent\n",
            "absent --advcl--> dropped\n",
            "lacking --xcomp--> absent\n",
            "sweeping --amod--> road\n",
            "epic --amod--> road\n",
            "fury --compound--> road\n",
            "road --dobj--> lacking\n",
            "almost --advmod--> gone\n",
            "gone --advcl--> went\n",
            "instead --advmod--> get\n",
            "get --dep--> went\n",
            "much --amod--> stuff\n",
            "background --compound--> stuff\n",
            "stuff --dobj--> get\n",
            "even --advmod--> sadly\n",
            "sadly --advmod--> get\n",
            "largely --advmod--> silent\n",
            "silent --amod--> sequence\n",
            "action --compound--> sequence\n",
            "sequence --nsubj--> lead\n",
            "lead --nsubj--> edge\n",
            "lack --compound--> tension\n",
            "tension --compound--> nothing\n",
            "nothing --dobj--> lead\n",
            "edge --nmod--> edit\n",
            "seat --dobj--> edge\n",
            "herethe --nmod--> edit\n",
            "writing --compound--> edit\n",
            "edit --nsubj--> is\n",
            "is --ccomp--> get\n",
            "nt --neg--> is\n",
            "great --amod--> bit\n",
            "character --compound--> lack\n",
            "lack --compound--> spark\n",
            "spark --compound--> bit\n",
            "bit --nsubj--> feel\n",
            "feel --ccomp--> is\n",
            "missing --xcomp--> feel\n",
            "miller --dobj--> missing\n",
            "trying --xcomp--> missing\n",
            "tell --compound--> story\n",
            "story --dobj--> trying\n",
            "seems --advcl--> went\n",
            "unsure --oprd--> seems\n",
            "write --compound--> character\n",
            "character --dobj--> unsure\n",
            "one --npadvmod--> let\n",
            "invest --amod--> telling\n",
            "furiosa --compound--> ambition\n",
            "ambition --compound--> story\n",
            "story --compound--> telling\n",
            "telling --nsubj--> think\n",
            "think --ccomp--> let\n",
            "execution --dobj--> think\n",
            "lacking --acl--> execution\n",
            "core --compound--> story\n",
            "story --dobj--> lacking\n",
            "solid --nsubj--> seems\n",
            "seems --ccomp--> think\n",
            "bit --dobj--> seems\n",
            "empty --amod--> time\n",
            "screen --compound--> film\n",
            "film --compound--> time\n",
            "jump --compound--> time\n",
            "time --npadvmod--> seems\n",
            "skipping --xcomp--> seems\n",
            "character --compound--> development\n",
            "development --dobj--> skipping\n",
            "worthy --amod--> edit\n",
            "explorationin --amod--> edit\n",
            "edit --dobj--> skipping\n",
            "really --advmod--> missed\n",
            "really --advmod--> missed\n",
            "missed --ccomp--> let\n",
            "fast --amod--> cut\n",
            "cut --compound--> miller\n",
            "fury --compound--> miller\n",
            "miller --nsubj--> built\n",
            "really --advmod--> built\n",
            "built --ccomp--> missed\n",
            "scene --nmod--> cut\n",
            "tight --amod--> cut\n",
            "production --compound--> cut\n",
            "cut --nsubj--> edits\n",
            "edits --ccomp--> built\n",
            "everything --nsubj--> lead\n",
            "build --acl--> everything\n",
            "beautifully --advmod--> lead\n",
            "lead --ccomp--> edits\n",
            "great --amod--> conclusion\n",
            "conclusion --compound--> scene\n",
            "miller --compound--> scene\n",
            "scene --compound--> scene\n",
            "scene --dobj--> lead\n",
            "fr --ccomp--> let\n",
            "whereas --mark--> mean\n",
            "furiosa --nsubj--> mean\n",
            "really --advmod--> lacking\n",
            "lacking --amod--> mean\n",
            "mean --advcl--> let\n",
            "there --advmod--> mean\n",
            "much --amod--> tension\n",
            "tension --dobj--> mean\n",
            "many --nsubj--> close\n",
            "close --amod--> momentsoverall\n",
            "amazing --amod--> momentsoverall\n",
            "momentsoverall --nsubj--> recognized\n",
            "like --prep--> momentsoverall\n",
            "furiosa --nmod--> road\n",
            "good --amod--> road\n",
            "mad --amod--> road\n",
            "max --compound--> film\n",
            "film --compound--> fury\n",
            "fury --compound--> road\n",
            "road --nmod--> film\n",
            "i --nmod--> film\n",
            "d --nmod--> film\n",
            "surprised --amod--> film\n",
            "film --nsubj--> recognized\n",
            "recognized --ccomp--> let\n",
            "oscar --compound--> time\n",
            "time --compound--> furiosa\n",
            "fury --compound--> road\n",
            "road --compound--> furiosa\n",
            "furiosa --nsubj--> get\n",
            "might --aux--> get\n",
            "get --ccomp--> recognized\n",
            "nomination --dobj--> get\n",
            "two --nummod--> nomination\n",
            "technical --amod--> nomination\n",
            "nomination --nsubj--> deed\n",
            "directing --acl--> nomination\n",
            "best --advmod--> pictureby\n",
            "pictureby --dobj--> directing\n",
            "deed --ccomp--> get\n",
            "honour --compound--> himby\n",
            "himby --compound--> wallet\n",
            "wallet --dobj--> deed\n",
            "honour --compound--> miller\n",
            "george --compound--> miller\n",
            "miller --compound--> witness\n",
            "witness --compound--> money\n",
            "furiosa --nmod--> money\n",
            "worth --amod--> money\n",
            "time --compound--> money\n",
            "money --npadvmod--> let\n",
            "\n",
            "Named Entity Recognition:\n",
            "Entity Summary:\n",
            "PERSON: 10\n",
            "ORG: 3\n",
            "GPE: 3\n",
            "CARDINAL: 2\n",
            "\n",
            "Extracted Named Entities:\n",
            "george miller - PERSON\n",
            "furiosa - ORG\n",
            "max - PERSON\n",
            "max - PERSON\n",
            "immortan - GPE\n",
            "taylor - PERSON\n",
            "furiosa hemsworth - PERSON\n",
            "immortan - GPE\n",
            "joe humungus presence wee - PERSON\n",
            "immortan - GPE\n",
            "joe - PERSON\n",
            "miller - ORG\n",
            "one - CARDINAL\n",
            "miller - ORG\n",
            "max - PERSON\n",
            "two - CARDINAL\n",
            "george miller - PERSON\n",
            "furiosa worth - PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VVY4yY-ksVM7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "# What I found challenging was the scraping of the data. Many sites have implemented anti-scraping systems that prevent large-scale data scraping, and overcoming this was a bit challenging. All the other parts of the assignment I found to be easier and more straightforward, especially the text cleaning and analysis processes. I enjoyed applying various natural language processing techniques to analyze the customer reviews, as it provided valuable insights into the sentiments expressed in the reviews.\n"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}