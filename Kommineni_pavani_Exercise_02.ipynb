{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanibasanth/pavani_INFO5731_Fall2024/blob/main/Kommineni_pavani_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\n",
        "\n",
        "# Research Question:\n",
        "research_question = \"Can heart rate and respiratory rate patterns predict early signs of stress in individuals during daily activities?\"\n",
        "\n",
        "# Data to Collect\n",
        "data_to_collect = {\n",
        "    \"Heart rate\": \"Captured via a wearable (smartwatch or fitness tracker).\",\n",
        "    \"Respiratory rate\": \"Measured by wearable sensors or smartphone apps.\",\n",
        "    \"Physical activity\": \"Captured using a phone's accelerometer (e.g., resting, walking, running).\",\n",
        "    \"Stress events\": \"Logged manually by participants through an app when they feel stressed.\",\n",
        "    \"Time of day\": \"Automatically recorded for temporal analysis.\"\n",
        "}\n",
        "\n",
        "# Amount of Data\n",
        "amount_of_data = {\n",
        "    \"Participants\": 30,\n",
        "    \"Duration\": \"2 weeks of continuous monitoring.\",\n",
        "    \"Heart rate and respiratory rate\": \"Recorded every minute.\",\n",
        "    \"Activity data\": \"Logged whenever a change is detected.\",\n",
        "    \"Stress events\": \"Manually logged as needed.\"\n",
        "}\n",
        "\n",
        "# Steps for Data Collection\n",
        "steps_for_data_collection = [\n",
        "    \"Recruit Participants: Find 30 volunteers willing to wear a smartwatch or sensor for 2 weeks and manually log stress events.\",\n",
        "    \"Setup Wearable Devices: Provide each participant with a smartwatch (or fitness tracker) that continuously records heart rate and respiratory rate. Install an app to track physical activity using the phone's accelerometer.\",\n",
        "    \"Manual Stress Logging: Set up a simple mobile app where participants can log moments when they feel stressed.\",\n",
        "    \"Data Recording: Continuously collect heart rate and respiratory rate every minute. Record activity level whenever there's a change. Time stamps will be automatically saved for all data points.\",\n",
        "    \"Store Data: Save data in CSV or JSON format locally or in the cloud.\"\n",
        "]\n",
        "\n",
        "# Example Data Format\n",
        "example_data_format = \"\"\"\n",
        "Timestamp, Heart Rate, Respiratory Rate, Activity Level, Stress Logged\n",
        "2024-09-13 09:00:00, 75, 16, Resting, No\n",
        "2024-09-13 09:01:00, 78, 17, Walking, No\n",
        "2024-09-13 09:02:00, 85, 18, Resting, Yes\n",
        "\"\"\"\n",
        "\n",
        "# Printing the research plan\n",
        "print(\"Research Question:\\n\", research_question)\n",
        "print(\"\\nData to Collect:\\n\", data_to_collect)\n",
        "print(\"\\nAmount of Data:\\n\", amount_of_data)\n",
        "print(\"\\nSteps for Data Collection:\\n\", \"\\n\".join(steps_for_data_collection))\n",
        "print(\"\\nExample Data Format:\\n\", example_data_format)\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abb57dc-3fc5-45a1-b0fd-a2fc9314aa86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research Question:\n",
            " Can heart rate and respiratory rate patterns predict early signs of stress in individuals during daily activities?\n",
            "\n",
            "Data to Collect:\n",
            " {'Heart rate': 'Captured via a wearable (smartwatch or fitness tracker).', 'Respiratory rate': 'Measured by wearable sensors or smartphone apps.', 'Physical activity': \"Captured using a phone's accelerometer (e.g., resting, walking, running).\", 'Stress events': 'Logged manually by participants through an app when they feel stressed.', 'Time of day': 'Automatically recorded for temporal analysis.'}\n",
            "\n",
            "Amount of Data:\n",
            " {'Participants': 30, 'Duration': '2 weeks of continuous monitoring.', 'Heart rate and respiratory rate': 'Recorded every minute.', 'Activity data': 'Logged whenever a change is detected.', 'Stress events': 'Manually logged as needed.'}\n",
            "\n",
            "Steps for Data Collection:\n",
            " Recruit Participants: Find 30 volunteers willing to wear a smartwatch or sensor for 2 weeks and manually log stress events.\n",
            "Setup Wearable Devices: Provide each participant with a smartwatch (or fitness tracker) that continuously records heart rate and respiratory rate. Install an app to track physical activity using the phone's accelerometer.\n",
            "Manual Stress Logging: Set up a simple mobile app where participants can log moments when they feel stressed.\n",
            "Data Recording: Continuously collect heart rate and respiratory rate every minute. Record activity level whenever there's a change. Time stamps will be automatically saved for all data points.\n",
            "Store Data: Save data in CSV or JSON format locally or in the cloud.\n",
            "\n",
            "Example Data Format:\n",
            " \n",
            "Timestamp, Heart Rate, Respiratory Rate, Activity Level, Stress Logged\n",
            "2024-09-13 09:00:00, 75, 16, Resting, No\n",
            "2024-09-13 09:01:00, 78, 17, Walking, No\n",
            "2024-09-13 09:02:00, 85, 18, Resting, Yes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the ranges for heart rate and respiratory rate\n",
        "HEART_RATE_MIN, HEART_RATE_MAX = 60, 120\n",
        "RESPIRATORY_RATE_MIN, RESPIRATORY_RATE_MAX = 12, 25\n",
        "ACTIVITY_LEVELS = [\"Resting\", \"Walking\", \"Running\", \"Sitting\"]\n",
        "\n",
        "# Function to generate random heart rate and respiratory rate\n",
        "def generate_sample():\n",
        "    heart_rate = random.randint(HEART_RATE_MIN, HEART_RATE_MAX)\n",
        "    respiratory_rate = random.randint(RESPIRATORY_RATE_MIN, RESPIRATORY_RATE_MAX)\n",
        "    activity_level = random.choice(ACTIVITY_LEVELS)\n",
        "    stress_logged = random.choice([\"Yes\", \"No\"])  # Simulating stress logging\n",
        "    return heart_rate, respiratory_rate, activity_level, stress_logged\n",
        "\n",
        "# Function to simulate 1000 samples\n",
        "def collect_data(num_samples=1000):\n",
        "    data = []\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        timestamp = (start_time + timedelta(minutes=i)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        heart_rate, respiratory_rate, activity_level, stress_logged = generate_sample()\n",
        "        data.append([timestamp, heart_rate, respiratory_rate, activity_level, stress_logged])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Save data to CSV file\n",
        "def save_data_to_csv(data, filename='heart_respiratory_data.csv'):\n",
        "    header = ['Timestamp', 'Heart Rate', 'Respiratory Rate', 'Activity Level', 'Stress Logged']\n",
        "\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Generate and save 1000 samples\n",
        "data = collect_data(1000)\n",
        "save_data_to_csv(data)\n",
        "\n",
        "print(\"Data collection complete. 1000 samples saved to 'heart_respiratory_data.csv'.\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d710923-9091-4d03-a8d4-8a7b43478a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection complete. 1000 samples saved to 'heart_respiratory_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBZIjVaZnAJU",
        "outputId": "13997fee-8e1f-4dcd-8c73-130ff1e8ee7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204ee1c2-5094-46c7-a99d-e442b1a6f0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles 1 to 100\n",
            "Fetching articles 101 to 200\n",
            "Error: 429\n",
            "100 articles saved to articles.csv\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Semantic Scholar API URL\n",
        "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Keyword for search\n",
        "KEYWORD = \"XYZ\"\n",
        "\n",
        "# Function to fetch articles from Semantic Scholar API\n",
        "def fetch_articles(keyword, year_range, offset=0, limit=100):\n",
        "    params = {\n",
        "        \"query\": keyword,\n",
        "        \"yearFilter\": f\"{year_range[0]}-{year_range[1]}\",\n",
        "        \"offset\": offset,\n",
        "        \"limit\": limit,\n",
        "        \"fields\": \"title,venue,year,authors,abstract\"\n",
        "    }\n",
        "    response = requests.get(API_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to collect 1000 articles\n",
        "def collect_articles(keyword, start_year, end_year, total_articles=1000):\n",
        "    articles = []\n",
        "    offset = 0\n",
        "    limit = 100  # Number of articles per API request\n",
        "\n",
        "    while len(articles) < total_articles:\n",
        "        print(f\"Fetching articles {len(articles) + 1} to {len(articles) + limit}\")\n",
        "        result = fetch_articles(keyword, (start_year, end_year), offset, limit)\n",
        "\n",
        "        if result and 'data' in result:\n",
        "            articles.extend(result['data'])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        offset += limit\n",
        "        time.sleep(1)  # Avoid making too many requests in a short time\n",
        "\n",
        "    return articles[:total_articles]\n",
        "\n",
        "# Function to save articles to a CSV file\n",
        "def save_articles_to_csv(articles, filename=\"articles.csv\"):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.get('title', 'N/A')\n",
        "            venue = article.get('venue', 'N/A')\n",
        "            year = article.get('year', 'N/A')\n",
        "            authors = \", \".join([author['name'] for author in article.get('authors', [])])\n",
        "            abstract = article.get('abstract', 'N/A')\n",
        "\n",
        "            writer.writerow([title, venue, year, authors, abstract])\n",
        "\n",
        "    print(f\"{len(articles)} articles saved to {filename}\")\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    start_year = 2014\n",
        "    end_year = 2024\n",
        "    total_articles = 1000\n",
        "\n",
        "    # Collect 1000 articles\n",
        "    articles = collect_articles(KEYWORD, start_year, end_year, total_articles)\n",
        "\n",
        "    # Save articles to CSV\n",
        "    save_articles_to_csv(articles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4efc51-049c-4e00-d215-b31d51658d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n",
            "An error occurred: 401 Unauthorized\n",
            "Unauthorized\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy pandas\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Replace these with your actual credentials\n",
        "BEARER_TOKEN = 'YOUR_BEARER_TOKEN'\n",
        "\n",
        "# Initialize Twitter API client\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Define the query\n",
        "query = '#python'\n",
        "\n",
        "# Fetch recent tweets\n",
        "try:\n",
        "    response = client.search_recent_tweets(query=query, max_results=10, tweet_fields=['created_at', 'public_metrics'])\n",
        "\n",
        "    if not response.data:\n",
        "        print(\"No tweets found for the given query.\")\n",
        "    else:\n",
        "        # Collect data\n",
        "        data = []\n",
        "        for tweet in response.data:\n",
        "            data.append({\n",
        "                'Text': tweet.text,\n",
        "                'Likes': tweet.public_metrics['like_count'],\n",
        "                'Retweets': tweet.public_metrics['retweet_count'],\n",
        "                'Created_At': tweet.created_at\n",
        "            })\n",
        "\n",
        "        # Save to CSV\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv('twitter_data.csv', index=False)\n",
        "        print(\"Data successfully saved to twitter_data.csv.\")\n",
        "\n",
        "except tweepy.TweepyException as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYqlYwfFgqrH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "\n",
        "\n",
        "Reflective Feedback on Web Scraping and Data Collection\n",
        "Learning Experience:\n",
        "\n",
        "I encountered several challenges while scraping certain websites, particularly those that employ anti-scraping techniques. For instance, Reddit's 403 and 401 errors were due to the site's security measures blocking the script. To overcome these challenges, I had to explore different approaches such as using authenticated API requests (e.g., using PRAW for Reddit) and learning how to use proxy rotation or more advanced techniques like asynchronous scraping to distribute the load and avoid detection.\n",
        "\n",
        "My overall learning experience in web scraping has been insightful. The key concepts that stood out included understanding how to interact with HTML elements on a webpage (like tags, attributes, and the DOM structure), using libraries like BeautifulSoup and requests to automate the extraction process, and handling potential issues such as dynamic content and pagination. One of the most beneficial techniques was learning how to simulate browser requests by setting proper headers (such as the User-Agent) to avoid being blocked by websites.\n",
        "\n",
        "Additionally, the importance of data cleaning and structuring became clear as I worked on saving scraped data in formats such as CSV. Managing rate limits and handling exceptions for potential issues like timeouts or missing data also proved valuable in making the process more robust.\n",
        "\n",
        "Challenges Encountered:\n",
        "I encountered several challenges while scraping certain websites, particularly those that employ anti-scraping techniques. For instance, Reddit's 403 and 401 errors were due to the site's security measures blocking the script. To overcome these challenges, I had to explore different approaches such as using authenticated API requests (e.g., using PRAW for Reddit) and learning how to use proxy rotation or more advanced techniques like asynchronous scraping to distribute the load and avoid detection.\n",
        "\n",
        "In other cases, websites required handling dynamically loaded content, for which JavaScript-based scraping tools like Selenium or APIs were essential. Navigating through such hurdles taught me the importance of understanding site behavior and respecting terms of service.\n",
        "\n",
        "Relevance to My Field of Study:\n",
        "In my field, the ability to gather and analyze data from online sources can significantly enhance research. Web scraping can be an essential tool for extracting real-time data from forums, research repositories, or social media platforms to identify trends or build datasets for further analysis. It offers the flexibility to gather large amounts of data quickly, which is useful in fields like cybersecurity, data science, or business analytics. Being able to automate this process ensures that data collection remains scalable and repeatable, making it a valuable skill for future projects.\n",
        "\n",
        "This exercise has strengthened my ability to work with data in various contexts and has equipped me with techniques that will be beneficial in research, especially when publicly available data needs to be leveraged.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "eef217cc-71e4-4194-d17d-f47f9ebb6535"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWrite your response here.\\n\\n\\n\\nReflective Feedback on Web Scraping and Data Collection\\nLearning Experience:\\n\\nI encountered several challenges while scraping certain websites, particularly those that employ anti-scraping techniques. For instance, Reddit's 403 and 401 errors were due to the site's security measures blocking the script. To overcome these challenges, I had to explore different approaches such as using authenticated API requests (e.g., using PRAW for Reddit) and learning how to use proxy rotation or more advanced techniques like asynchronous scraping to distribute the load and avoid detection.\\n\\nMy overall learning experience in web scraping has been insightful. The key concepts that stood out included understanding how to interact with HTML elements on a webpage (like tags, attributes, and the DOM structure), using libraries like BeautifulSoup and requests to automate the extraction process, and handling potential issues such as dynamic content and pagination. One of the most beneficial techniques was learning how to simulate browser requests by setting proper headers (such as the User-Agent) to avoid being blocked by websites.\\n\\nAdditionally, the importance of data cleaning and structuring became clear as I worked on saving scraped data in formats such as CSV. Managing rate limits and handling exceptions for potential issues like timeouts or missing data also proved valuable in making the process more robust.\\n\\nChallenges Encountered:\\nI encountered several challenges while scraping certain websites, particularly those that employ anti-scraping techniques. For instance, Reddit's 403 and 401 errors were due to the site's security measures blocking the script. To overcome these challenges, I had to explore different approaches such as using authenticated API requests (e.g., using PRAW for Reddit) and learning how to use proxy rotation or more advanced techniques like asynchronous scraping to distribute the load and avoid detection.\\n\\nIn other cases, websites required handling dynamically loaded content, for which JavaScript-based scraping tools like Selenium or APIs were essential. Navigating through such hurdles taught me the importance of understanding site behavior and respecting terms of service.\\n\\nRelevance to My Field of Study:\\nIn my field, the ability to gather and analyze data from online sources can significantly enhance research. Web scraping can be an essential tool for extracting real-time data from forums, research repositories, or social media platforms to identify trends or build datasets for further analysis. It offers the flexibility to gather large amounts of data quickly, which is useful in fields like cybersecurity, data science, or business analytics. Being able to automate this process ensures that data collection remains scalable and repeatable, making it a valuable skill for future projects.\\n\\nThis exercise has strengthened my ability to work with data in various contexts and has equipped me with techniques that will be beneficial in research, especially when publicly available data needs to be leveraged.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}